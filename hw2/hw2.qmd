---
title: "Biostat 200C Homework 2"
author: "Kiana Mohammadinik"
subtitle: Due Apr 25 @ 11:59PM
date: today
format:
 pdf:
    theme: cosmo
    embed-resources: true
    number-sections: true
    toc: true
    colorlinks: true
engine: knitr
knitr:
  opts_chunk: 
    fig.align: 'center'
    # fig.width: 6
    # fig.height: 4
    message: FALSE
    cache: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = FALSE)
```

To submit homework, please upload both Rmd and html files to Bruinlearn by the deadline.

## Q1. CFR of COVID-19 (90pts)

Of primary interest to public is the risk of dying from COVID-19. A commonly used measure is case fatality rate/ratio/risk (CFR), which is defined as $$
\frac{\text{number of deaths from disease}}{\text{number of diagnosed cases of disease}}.
$$ Apparently CFR is not a fixed constant; it changes with time, location, and other factors. Also CFR is different from the infection fatality rate (IFR), the probability that someone infected with COVID-19 dies from it.

In this exercise, we use logistic regression to study how US county-level CFR changes according to demographic information and some health-, education-, and economy-indicators.

### Data sources

-   `04-04-2020.csv.gz`: The data on COVID-19 confirmed cases and deaths on 2020-04-04 is retrieved from the [Johns Hopkins COVID-19 data repository](https://github.com/CSSEGISandData/COVID-19). It was downloaded from this [link](https://github.com/CSSEGISandData/COVID-19) (commit 0174f38). This repository has been archived by the owner on Mar 10, 2023. It is now read-only. You can download data from box: <https://ucla.box.com/s/brb3vz4nwoq8pjkcutxncymqw583d39l>

-   `us-county-health-rankings-2020.csv.gz`: The 2020 County Health Ranking Data was released by [County Health Rankings](https://www.countyhealthrankings.org). The data was downloaded from the [Kaggle Uncover COVID-19 Challenge](https://www.kaggle.com/roche-data-science-coalition/uncover) (version 1). You can download data from box: <https://ucla.box.com/s/brb3vz4nwoq8pjkcutxncymqw583d39l>

### data preparation

Load the `tidyverse` package for data manipulation and visualization.

```{r}
# tidyverse of data manipulation and visualization
library(tidyverse)
```

Read in the data of COVID-19 cases reported on 2020-04-04.

```{r}
county_count <- read_csv("04-04-2020.csv") %>%
  # cast fips into dbl for use as a key for joining tables
  mutate(FIPS = as.numeric(FIPS)) %>%
  filter(Country_Region == "US") %>%
  print(width = Inf)
```

Standardize the variable names by changing them to lower case.

```{r}
names(county_count) <- str_to_lower(names(county_count))
```

Sanity check by displaying the unique US states and territories:

```{r}
county_count %>%
  select(province_state) %>%
  distinct() %>%
  arrange(province_state) %>%
  print(n = Inf)
```

We want to exclude entries from `Diamond Princess`, `Grand Princess`, `Guam`, `Northern Mariana Islands`, `Puerto Rico`, `Recovered`, and `Virgin Islands`, and only consider counties from 50 states and DC.

```{r}
county_count <- county_count %>%
  filter(!(province_state %in% c("Diamond Princess", "Grand Princess", 
                                 "Recovered", "Guam", "Northern Mariana Islands", 
                                 "Puerto Rico", "Virgin Islands"))) %>%
  print(width = Inf)
```

Graphical summarize the COVID-19 confirmed cases and deaths on 2020-04-04 by state.

```{r}
county_count %>%
  # turn into long format for easy plotting
  pivot_longer(confirmed:recovered, 
               names_to = "case", 
               values_to = "count") %>%
  group_by(province_state) %>%
  ggplot() + 
  geom_col(mapping = aes(x = province_state, y = `count`, fill = `case`)) + 
  # scale_y_log10() + 
  labs(title = "US COVID-19 Situation on 2020-04-04", x = "State") + 
  theme(axis.text.x = element_text(angle = 90))
```

Read in the 2020 county-level health ranking data.

```{r}
county_info <- read_csv("us-county-health-rankings-2020.csv") %>%
  filter(!is.na(county)) %>%
  # cast fips into dbl for use as a key for joining tables
  mutate(fips = as.numeric(fips)) %>%
  select(fips, 
         state,
         county,
         percent_fair_or_poor_health, 
         percent_smokers, 
         percent_adults_with_obesity, 
         # food_environment_index,
         percent_with_access_to_exercise_opportunities, 
         percent_excessive_drinking,
         # teen_birth_rate, 
         percent_uninsured,
         # primary_care_physicians_rate,
         # preventable_hospitalization_rate,
         # high_school_graduation_rate,
         percent_some_college,
         percent_unemployed,
         percent_children_in_poverty,
         # `80th_percentile_income`,
         # `20th_percentile_income`,
         percent_single_parent_households,
         # violent_crime_rate,
         percent_severe_housing_problems,
         overcrowding,
         # life_expectancy,
         # age_adjusted_death_rate,
         percent_adults_with_diabetes,
         # hiv_prevalence_rate,
         percent_food_insecure,
         # percent_limited_access_to_healthy_foods,
         percent_insufficient_sleep,
         percent_uninsured_2,
         median_household_income,
         average_traffic_volume_per_meter_of_major_roadways,
         percent_homeowners,
         # percent_severe_housing_cost_burden,
         population_2,
         percent_less_than_18_years_of_age,
         percent_65_and_over,
         percent_black,
         percent_asian,
         percent_hispanic,
         percent_female,
         percent_rural) %>%
  print(width = Inf)
```

For stability in estimating CFR, we restrict to counties with $\ge 5$ confirmed cases.

```{r}
county_count <- county_count %>%
  filter(confirmed >= 5)
```

We join the COVID-19 count data and county-level information using FIPS (Federal Information Processing System) as key.

```{r}
county_data <- county_count %>%
  left_join(county_info, by = "fips") %>%
  print(width = Inf)
```

Numerical summaries of each variable:

```{r}
summary(county_data)
```

List rows in `county_data` that don't have a match in `county_count`:

```{r}
county_data %>%
  filter(is.na(state) & is.na(county)) %>%
  print(n = Inf)
```

We found there are some rows that miss `fips`.

```{r}
county_count %>%
  filter(is.na(fips)) %>%
  select(fips, admin2, province_state) %>%
  print(n = Inf)
```

We need to (1) manually set the `fips` for some counties, (2) discard those `Unassigned`, `unassigned` or `Out of`, and (3) try to join with `county_info` again.

```{r}
county_data <- county_count %>%
  # manually set FIPS for some counties
  mutate(fips = ifelse(admin2 == "DeKalb" & province_state == "Tennessee", 47041, fips)) %>%
  mutate(fips = ifelse(admin2 == "DeSoto" & province_state == "Florida", 12027, fips)) %>%
  #mutate(fips = ifelse(admin2 == "Dona Ana" & province_state == "New Mexico", 35013, fips)) %>% 
  mutate(fips = ifelse(admin2 == "Dukes and Nantucket" & province_state == "Massachusetts", 25019, fips)) %>% 
  mutate(fips = ifelse(admin2 == "Fillmore" & province_state == "Minnesota", 27045, fips)) %>%  
  #mutate(fips = ifelse(admin2 == "Harris" & province_state == "Texas", 48201, fips)) %>%  
  #mutate(fips = ifelse(admin2 == "Kenai Peninsula" & province_state == "Alaska", 2122, fips)) %>%  
  mutate(fips = ifelse(admin2 == "LaSalle" & province_state == "Illinois", 17099, fips)) %>%
  #mutate(fips = ifelse(admin2 == "LaSalle" & province_state == "Louisiana", 22059, fips)) %>%
  #mutate(fips = ifelse(admin2 == "Lac qui Parle" & province_state == "Minnesota", 27073, fips)) %>%  
  mutate(fips = ifelse(admin2 == "Manassas" & province_state == "Virginia", 51683, fips)) %>%
  #mutate(fips = ifelse(admin2 == "Matanuska-Susitna" & province_state == "Alaska", 2170, fips)) %>%
  mutate(fips = ifelse(admin2 == "McDuffie" & province_state == "Georgia", 13189, fips)) %>%
  #mutate(fips = ifelse(admin2 == "McIntosh" & province_state == "Georgia", 13191, fips)) %>%
  #mutate(fips = ifelse(admin2 == "McKean" & province_state == "Pennsylvania", 42083, fips)) %>%
  mutate(fips = ifelse(admin2 == "Weber" & province_state == "Utah", 49057, fips)) %>%
  filter(!(is.na(fips) | str_detect(admin2, "Out of") | str_detect(admin2, "Unassigned"))) %>%
  left_join(county_info, by = "fips") %>%
  print(width = Inf)
```

Summarize again

```{r}
summary(county_data)
```

If there are variables with missing value for many counties, we go back and remove those variables from consideration.

Let's create a final data frame for analysis.

```{r}
county_data <- county_data %>%
  mutate(state = as.factor(state)) %>%
  select(county, confirmed, deaths, state, percent_fair_or_poor_health:percent_rural)
summary(county_data)
```

Display the 10 counties with highest CFR.

```{r}
county_data %>%
  mutate(cfr = deaths / confirmed) %>%
  select(county, state, confirmed, deaths, cfr) %>%
  arrange(desc(cfr)) %>%
  top_n(10)
```

Write final data into a csv file for future use.

```{r}
write_csv(county_data, "covid19-county-data-20200404.csv")
```

### Note:

Given that the datasets were collected in the middle of the pandemic, what assumptions of CFR might be violated by defining CFR as `deaths/confirmed` from this data set?

Because COVID-19 pandemic was still ongoing in 2020, we should realize some critical assumptions for defining CFR are not met using this datasets.

1.  Numbers of confirmed cases do not reflect the number of diagnosed people. This is mainly limited by the availability of testing.

2.  Some confirmed cases may die later.

With acknowledgement of these severe limitations, we continue to use `deaths/confirmed` as a very rough proxy of CFR.

### Q1.1 (5pts)

Read and run above code to generate a data frame `county_data` that includes county-level COVID-19 confirmed cases and deaths, demographic, and health related information.

### Q1.2(5pts)

What assumptions of logistic regression may be violated by this data set? Logistic regression assumes that the log-odds of the outcome is linearly related to the predictors, that observations are independent, and that there is no multicollinearity among predictors, but our dataset may violate some of these assumptions in several ways. First, observations (counties) may not be fully independent due to the shared state/federal-level policies or geographic proximity. Second, some predictors might have nonlinear relationships with the outcome, violating the linearity of the logit. Third, strong correlations among socioeconomic and demographic variables may lead to multicollinearity. Also, counties with very small populations or extreme CFRs may have extra undue influence on the model and lead to high-leverage or outlier effects.

### Q1.3 (10pts)

Run a logistic regression, using variables `state`, ..., `percent_rural` as predictors. **Solution:**

```{r}
library(broom) 
# Logistic regression binomial
full_model <- glm(cbind(deaths, confirmed - deaths) ~ state + percent_fair_or_poor_health + percent_smokers + percent_adults_with_obesity + percent_with_access_to_exercise_opportunities + percent_excessive_drinking + percent_uninsured + percent_some_college + percent_unemployed + percent_children_in_poverty + percent_single_parent_households + percent_severe_housing_problems + overcrowding + percent_adults_with_diabetes + percent_food_insecure + percent_insufficient_sleep + percent_uninsured_2 + median_household_income + average_traffic_volume_per_meter_of_major_roadways + percent_homeowners + population_2 + percent_less_than_18_years_of_age + percent_65_and_over + percent_black + percent_asian + percent_hispanic + percent_female + percent_rural, family = binomial(), data = county_data)
```

### Q1.4 (10pts)

Interpret the regression coefficients of 3 significant predictors with p-value \<0.05. **Solution:**

```{r}
tidy(full_model) %>% 
  arrange(p.value) %>% 
  filter(p.value < 0.05)
```

percent_hispanic: For each 1 percentage point increase in the percent of Hispanic residents in a county, the log-odds of COVID-19 death among confirmed cases decreases by 0.0192, holding other variables constant.\
Converting to odds ratio: exp(-0.0192) ≈ 0.981\
So, the odds of death decrease by \~1.9% for every 1% increase in the Hispanic population.\

percent_insufficient_sleep: For every 1% increase in the rate of insufficient sleep (adults who report not getting enough sleep), the log-odds of death increase by 0.0444.\
Converting to odds ratio: exp(0.0444) ≈ 1.0454\
So, the odds of death increase by about 4.54% for every 1% increase in the rate of insufficient sleep.\

median_household_income: For each \$1 increase in median household income, the log-odds of death decrease by -1.084392e-05.\
Convert to a meaningful unit like \$10,000: 10000 \* -1.084392e-05 = -0.1084\
Converting to odds ratio: exp(-0.1084) ≈ 0.897\
So, the odds of COVID-19 death decrease by about 10.3% for every \$10,000 increase in median household income.\

### Q1.5 (10pts)

Apply analysis of deviance to (1) evaluate the goodness of fit of the model and (2) compare the model to the intercept-only model. **Solution:**

```{r}
null_model <- glm(cbind(deaths, confirmed - deaths) ~ 1,
                  family = binomial(),
                  data = county_data)

anova(null_model, full_model, test = "Chisq")

# Extract residual deviance and degrees of freedom
res_dev <- deviance(full_model)
res_df <- df.residual(full_model)

# Perform chi-squared goodness-of-fit test
p_value <- pchisq(res_dev, df = res_df, lower.tail = FALSE)

# Output
cat("Residual deviance:", res_dev, "\n")
cat("Residual degrees of freedom:", res_df, "\n")
cat("Goodness-of-fit p-value:", p_value, "\n")
```

Model Comparison:\
The test returned a p-value of 2.2e-16, indicating that the full model provides a significantly better fit to the data than the null model.\

Goodness-of-Fit:\
The residual deviance of the full model is 1842.755, and the residual degrees of freedom is 1368. The Goodness-of-fit p-value is extremely small (1.099787e-16), suggesting that the model may not fit the data perfectly .

### Q1.6 (10pts)

Perform analysis of deviance to evaluate the significance of each predictor. Display the 10 most significant predictors. **Solution:**

```{r}
library(car)
# Type II Analysis of Deviance 
deviance_table <- Anova(full_model, type = "II", test = "LR")

# Pull out the top 10 by p‐value
top_predictors <- as.data.frame(deviance_table) %>%
  rownames_to_column("Predictor") %>%
  arrange(`Pr(>Chisq)`) %>%  
  slice(1:10)

print(top_predictors)
```

### Q1.7 (5pts)

Construct confidence intervals of regression coefficients. **Solution:**

```{r, warning=FALSE}
confint(full_model)
```

```{r}
# ALTERNATIVE SOLUTION (commented out)
# library(broom)
# library(tidyverse)

# Profile-likelihood CIs throws out warnings
# ci_profile <- confint(full_model) %>% 
#  as_tibble(rownames = "term") %>% 
#  rename(profile_low = `2.5 %`, profile_high = `97.5 %`)

# Wald CIs (faster approximation)
# ci_wald <- tidy(full_model, conf.int = TRUE, conf.level = 0.95) %>% 
#  select(term, estimate, std.error, 
#        wald_low = conf.low, wald_high = conf.high, 
#        p.value)

# Combine results
# full_results <- ci_wald %>% 
#  left_join(ci_profile, by = "term") %>% 
#  arrange(p.value)

# Display top predictors
# full_results %>% 
#  head(10) %>% 
#  knitr::kable(digits = 4)
```

### Q1.8 (5pts)

Plot the deviance residuals against the fitted values. Are there potential outliers? **Solution:**

```{r}
library(ggplot2)

# Extract deviance residuals and fitted probabilities
resid_dev   <- residuals(full_model, type = "deviance")
fitted_prob <- fitted(full_model)

# Build a data frame
plot_df <- data.frame(
  fitted = fitted_prob,
  dev_resid = resid_dev
)

# 3. Plot
ggplot(plot_df, aes(x = fitted, y = dev_resid)) +
  geom_point(alpha = 0.4) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  # highlight points with |resid| > 2
  geom_point(data = subset(plot_df, abs(dev_resid) > 2),
             color = "red", size = 1) +
  labs(
    title = "Deviance Residuals vs Fitted Values",
    x     = "Fitted Probability of Death",
    y     = "Deviance Residual"
  ) +
  theme_minimal()
```

### Q1.9 (5pts)

Plot the half-normal plot. Are there potential outliers in predictor space? **Solution:**

```{r}
library(faraway)
halfnorm(hatvalues(full_model))
```

Based on the plot, the points 931 and 367 are potential outliers in predictor space.\

```{r}
# ALTERNATIVE SOLUTION (commented out)
# Compute leverage (hat) values
# lev  <- hatvalues(full_model)
# n    <- length(lev)
# lev_ord <- sort(lev)

# hnq <- qnorm( (2*seq_len(n) - 1) / (2*n) )   # half-normal
# hnq <- sort(abs(hnq))

# df <- data.frame(hnq, lev = lev_ord)

#ggplot(df, aes(hnq, lev)) +
#  geom_point(alpha = .6) +
#  geom_abline(intercept = 0,
#              slope = sum(lev_ord*hnq)/sum(hnq^2),
#              linetype = "dashed") +
#  labs(x = "Half-normal quantiles",
 #      y = "Leverage (hat-values)",
#       title = "Half-Normal plot of leverage")


# Identify high-leverage points
# p <- length(coef(full_model))         
# n  <- nobs(full_model)           
# thresh <- 2 * (p + 1) / n

# lev <- hatvalues(full_model)
# high_lev_idx <- which(lev > thresh)

# View the top offenders
# data.frame(
#  county = county_data$county[high_lev_idx],
#  state  = county_data$state[high_lev_idx],
#  leverage = lev[high_lev_idx]
# ) %>%
#  arrange(desc(leverage)) %>%
#  head(10)
```

### Q1.10 (10pts)

Find the best sub-model using the AIC criterion. **Solution:**

```{r}
library(MASS)

# Run stepwise AIC (both directions)
step_model <- stepAIC(
  object    = full_model,
  direction = "both",
  trace     = FALSE      
)

# Show the final selected formula
cat("Selected model formula:\n")
print(formula(step_model))

# Report its AIC
cat("\nAIC of selected model:", AIC(step_model), "\n")

# Inspect coefficient estimates
summary(step_model)
```

Best AIC model: cbind(deaths, confirmed - deaths) \~ state + percent_fair_or_poor_health + percent_smokers + percent_excessive_drinking + percent_uninsured + percent_some_college + percent_unemployed + percent_children_in_poverty + percent_single_parent_households + percent_severe_housing_problems + percent_adults_with_diabetes + percent_food_insecure + percent_insufficient_sleep + percent_uninsured_2 + median_household_income + percent_homeowners + population_2 + percent_less_than_18_years_of_age + percent_black + percent_hispanic + percent_female

Final AIC = 3870.06, a drop of \~10.6 units from the full model.\

### Q1.11 (15pts)

Find the best sub-model using the lasso with cross validation. **Solution:**

```{r}
library(glmnet)

# Build the design matrix and response
x <- model.matrix(
  deaths/confirmed ~ state
    + percent_fair_or_poor_health
    + percent_smokers
    + percent_adults_with_obesity
    + percent_with_access_to_exercise_opportunities
    + percent_excessive_drinking
    + percent_uninsured
    + percent_some_college
    + percent_unemployed
    + percent_children_in_poverty
    + percent_single_parent_households
    + percent_severe_housing_problems
    + overcrowding
    + percent_adults_with_diabetes
    + percent_food_insecure
    + percent_insufficient_sleep
    + percent_uninsured_2
    + median_household_income
    + average_traffic_volume_per_meter_of_major_roadways
    + percent_homeowners
    + population_2
    + percent_less_than_18_years_of_age
    + percent_65_and_over
    + percent_black
    + percent_asian
    + percent_hispanic
    + percent_female
    + percent_rural,
  data = county_data
)[, -1]

## grouped-binomial response
y <- cbind(
  deaths     = county_data$deaths,
  survivors  = county_data$confirmed - county_data$deaths
)

## 10-fold CV lasso
set.seed(123)
cv_lasso <- cv.glmnet(
  x, y,
  family        = "binomial",  
  type.measure  = "deviance"
)

## visualise & extract coefficients 
plot(cv_lasso)                      
abline(v = log(c(cv_lasso$lambda.min,
                 cv_lasso$lambda.1se)),
       lty = 2)

best_coefs <- coef(cv_lasso, s = "lambda.min")
nonzero    <- which(best_coefs != 0)

lasso_results <- data.frame(
  term     = rownames(best_coefs)[nonzero],
  estimate = as.numeric(best_coefs[nonzero])
)
print(lasso_results, row.names = FALSE)
```

Since CV lasso selects a model with just the intercept, we can check the coefficients at the next 5 smaller lambdas (more predictors).

```{r}
# index of lambda.min on the path 
k_min <- match(cv_lasso$lambda.min, cv_lasso$glmnet.fit$lambda)

# pick the next 5 smaller lambdas (more predictors)
k_seq <- k_min + 0:5                           
k_seq <- k_seq[k_seq <= length(cv_lasso$glmnet.fit$lambda)]

lambda_seq <- cv_lasso$glmnet.fit$lambda[k_seq]

results <- lapply(lambda_seq, function(lam) {
  beta <- coef(cv_lasso$glmnet.fit, s = lam)
  nz   <- which(beta != 0)
  data.frame(
    lambda      = lam,
    n_nonzero   = length(nz),
    cv_deviance = cv_lasso$cvm[match(lam, cv_lasso$lambda)],
    term        = rownames(beta)[nz],
    estimate    = as.numeric(beta[nz]),
    row.names   = NULL
  )
})

results_df <- do.call(rbind, results)

# show one table per lambda
library(dplyr)
results_df %>%
  group_by(lambda, n_nonzero, cv_deviance) %>%
  summarise(terms = paste(term, collapse = ", "),
            .groups = "drop") %>%
  arrange(lambda) %>%
  knitr::kable(digits = 4)
```

## Q2. Odds ratios (20pts)

Consider a $2 \times 2$ contingency table from a prospective study in which people who were or were not exposed to some pollutant are followed up and, after several years, categorized according to the presense or absence of a disease. Following table shows the probabilities for each cell. The odds of disease for either exposure group is $O_i = \pi_i / (1 - \pi_i)$, for $i = 1,2$, and so the odds ratio is $$
\phi = \frac{O_1}{O_2} = \frac{\pi_1(1 - \pi_2)}{\pi_2 (1 - \pi_1)}
$$ is a measure of the relative likelihood of disease for the exposed and not exposed groups.

|             | Diseased | Not diseased |
|:-----------:|----------|--------------|
|   Exposed   | $\pi_1$  | $1 - \pi_1$  |
| Not exposed | $\pi_2$  | $1 - \pi_2$  |

### Q2.1 (10pts)

For the simple logistic model $$
\pi_i = \frac{e^{\beta_i}}{1 + e^{\beta_i}}, 
$$ show that if there is no difference between the exposed and not exposed groups (i.e., $\beta_1 = \beta_2$), then $\phi = 1$.

**Solution:** 

The odds for group $i$ are  

$$
\begin{aligned}
O_i
  &= \frac{\pi_i}{1 - \pi_i} \\[4pt]
  &= \frac{\dfrac{e^{\beta_i}}{1 + e^{\beta_i}}}
          {1 - \dfrac{e^{\beta_i}}{1 + e^{\beta_i}}} \\[8pt]
  &= \frac{e^{\beta_i}}{1 + e^{\beta_i}}
     \,\cdot\,
     \frac{1 + e^{\beta_i}}{1}
     \;=\;
     e^{\beta_i}.
\end{aligned}
$$

So the odds-ratio is  

$$
\phi
   \;=\;
   \frac{O_1}{O_2}
   \;=\;
   \frac{e^{\beta_1}}{e^{\beta_2}}
   \;=\;
   e^{\beta_1 - \beta_2}.
$$

If $\beta_1 = \beta_2$, then  

$$
\phi
  \;=\;
  e^{\,\beta_1 - \beta_1}
  \;=\;
  e^{0}
  \;=\;
  1
$$





### Q2.2(10pts)

Consider $J$ $2 \times 2$ tables, one for each level $x_j$ of a factor, such as age group, with $j=1,\ldots, J$. For the logistic model $$
\pi_{ij} = \frac{e^{\alpha_i + \beta_i x_j}}{1 + e^{\alpha_i + \beta_i x_j}}, \quad i = 1,2, \quad j= 1,\ldots, J.
$$ Show that $\log \phi$ is constant over all tables if $\beta_1 = \beta_2$.

**Solution:**

Given  

$$
\pi_{ij} \;=\; \frac{e^{\alpha_i + \beta_i x_j}}{1 + e^{\alpha_i + \beta_i x_j}}
$$

the odds for group \(i\) at level \(x_j\) are  

$$
O_{ij}
  \;=\;
  \frac{\pi_{ij}}{1 - \pi_{ij}}
  \;=\;
  e^{\alpha_i + \beta_i x_j}.
$$

The odds-ratio for each \(j\) is  

$$
\phi_j
  \;=\;
  \frac{O_{1j}}{O_{2j}}
  \;=\;
  \frac{e^{\alpha_1 + \beta_1 x_j}}
       {e^{\alpha_2 + \beta_2 x_j}}
  \;=\;
  e^{(\alpha_1 - \alpha_2) + (\beta_1 - \beta_2)\,x_j}.
$$

So the log-odds-ratio is  

$$
\log \phi_j
  \;=\;
  (\alpha_1 - \alpha_2)
  \;+\;
  (\beta_1 - \beta_2)\,x_j.
$$

If \(\beta_1 = \beta_2\), then  

$$
\log \phi_j
  \;=\;
  (\alpha_1 - \alpha_2),
$$

which no longer depends on \(x_j\). We can conclude the log-odds-ratio is **constant** across all \(j\).

## Q3. ELMR Chapter 4 Excercise 3 (30pts)

The infert dataset presents data from a study of secondary infertility (failure to conceive after at least one previous conception). The factors of interest are induced abortions and spontaneous abortions (e.g., miscarriages). The study matched each case of infertility with two controls who were not infertile, matching on age, education and parity (number of prior pregnancies).\
(a) Construct cross-classified tables by number of spontaneous and induced abortions separately for cases and controls. Comment on the differences between the two tables.\
(b) Fit a binary response model with only spontaneous and induced as predictors.\
Determine the statistical significance of these predictors. Express the effects of the predictors in terms of odds.\
(c) Fit a binary response model with only education, age and parity as predictors.\
Explain how the significance (or lack thereof) of these predictors should be interpreted.\
(d) Now put all five predictors in a binary response model. Interpret the results in terms of odds.\
(e) Fit a matched case control model appropriate to the data. Interpret the output and compare the odds to those found in the previous model.\
(f) The spontaneous and induced predictors could be viewed as ordinal due to the grouping in the highest level. Refit the model using ordinal factors rather than numerical variables for these two predictors. Is there evidence that the ordinal representation is necessary?\

### Q3.a

```{r}

```

### Q3.b

```{r}

```

### Q3.c

```{r}

```

### Q3.d

```{r}

```

### Q3.e

```{r}

```

### Q3.f

```{r}

```
