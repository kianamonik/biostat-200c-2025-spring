---
title: "Biostat 200C Homework 1"
subtitle: Due Apr 11 @ 11:59PM
date: today
format:
  html:
    theme: cosmo
    embed-resources: true
    number-sections: true
    toc: true
    toc-depth: 4
    toc-location: left
    code-fold: false
engine: knitr
knitr:
  opts_chunk: 
    fig.align: 'center'
    # fig.width: 6
    # fig.height: 4
    message: FALSE
    cache: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = FALSE)
```

To submit homework, please submit Rmd and html files to bruinlearn by the deadline.

```{r}
library(tidyverse)
library(GGally)
library(corrplot)
library(gtsummary)
library(knitr)
library(faraway)
library(gridExtra)
library(ggstatsplot)
library(MASS)
library(broom)
library(ggfortify)
```

## Q1. Reivew of linear models (60pts)
The swiss data — use Fertility as the response to practice

### Q1.1

An initial data analysis that explores the numerical and graphical characteristics of the data.(5pts)

**Solution:**
```{r, warning=FALSE, message=FALSE}
data("swiss")
str(swiss)

# Summary statistics
summary(swiss)

# Correlation matrix
ggcorrmat(
  data = swiss,
  cor.vars = colnames(swiss),
  title = "Correlation Matrix with Significance Tests"
)

# Pairwise plots
ggpairs(
  swiss,
  lower = list(continuous = wrap("smooth", alpha = 0.3, color = "darkgreen")),
  diag = list(continuous = wrap("densityDiag", fill = "skyblue", alpha = 0.6)),
  upper = list(continuous = wrap("cor", size = 4))
)

# Boxplot for Fertility
boxplot(swiss)
```
### Q1.2

Variable selection to choose the best model. (10pts)
**Solution:**
```{r}
# Full linear model
full_model <- lm(Fertility ~ ., data = swiss)

summary(full_model)

# Perform stepwise AIC-based selection 
step_model <- step(full_model, direction = "both", trace = TRUE)

# Summary of the final selected model
summary(step_model)

# Check the significance of each variable in the selected model
drop1(step_model, test = "F")
```
Final model: Fertility ~ Agriculture + Education + Catholic + Infant.Mortality

### Q1.3

An exploration of transformations to improve the fit of the model. (10pts)
**Solution:**
```{r}
# Check for linearity 
swiss %>%
  pivot_longer(cols = -Fertility) %>%
  ggplot(aes(x = value, y = Fertility)) +
  geom_point(alpha = 0.6) +
  geom_smooth(method = "lm", se = FALSE, color = "steelblue") +
  facet_wrap(~ name, scales = "free_x") +
  theme_minimal() +
  labs(title = "Fertility vs. Predictors (Raw Scale)")

# Add transformed variable
swiss_trans <- swiss %>%
  mutate(log_Catholic = log(Catholic + 1))


# Fit model with log-transformed Catholic
model_trans <- lm(Fertility ~ Agriculture + Education + log_Catholic +
                    Infant.Mortality, data = swiss_trans)

# Compare with previous model
AIC(step_model, model_trans)
summary(model_trans)

# plot termplots side by side
par(mfrow=c(2,2))
termplot(step_model, partial.resid = TRUE, terms = 3)
termplot(model_trans, partial.resid = TRUE, terms = 3)

# polynomial transformation for Catholic
model_poly <- lm(Fertility ~ Agriculture + poly(Catholic, 2) + I(Education^2) +
                   Infant.Mortality, data = swiss)
AIC(step_model, model_poly)

# square root transformation for Catholic
model_sqrt <- lm(Fertility ~ Agriculture + sqrt(Catholic) + Education +
                   Infant.Mortality, data = swiss)
AIC(step_model, model_sqrt)

# inverse for Catholic
model_inv <- lm(Fertility ~ Agriculture + I(1 / (Catholic + 1)) + Education +
                  Infant.Mortality, data = swiss)
AIC(step_model, model_inv)

```

The original model (without transformation) has the lowest AIC, meaning it fits the data better despite the skew in the Catholic variable.

### Q1.4

Diagnostics to check the assumptions of your model. (10pts)
**Solution:**
```{r}
final_model <- step_model 

par(mfrow = c(2, 2))
plot(final_model)

# Reset graphic parameters
par(mfrow = c(1, 1))
```
The residuals show no major nonlinear patterns or heteroscedasticity issues. The QQ plot indicates that the residuals are approximately normally distributed, and the scale-location plot shows a random scatter of points, indicating homoscedasticity. No highly influential points are present based on leverage and Cook’s distance. The model appears to be a relatively good fit for the data based on these diagnostics.


### Q1.5
Some predictions of future observations for interesting values of the predictors.(5pts)
**Solution:**
```{r}
# Quantile values for predictors
catholic_vals <- quantile(swiss$Catholic, c(0.25, 0.75))
edu_vals <- quantile(swiss$Education, c(0.25, 0.75))

# Create prediction grid
pred_df <- tibble(
  Catholic = rep(catholic_vals, each = 2),
  Education = rep(edu_vals, times = 2),
  Agriculture = mean(swiss$Agriculture),
  Infant.Mortality = mean(swiss$Infant.Mortality)
)

# Generate predictions + 95% prediction intervals
pred_output <- predict(final_model, newdata = pred_df, interval = "prediction")

# Combine with input
pred_results <- bind_cols(pred_df, as_tibble(pred_output))

# View results
pred_results

xtabs(round(fit, 1) ~ Education + Catholic, data = pred_results)
```
Fertility for the 25th percentile of Catholic and 25th percentile of Education is 70.5, while for the 75th percentile of Catholic and 75th percentile of Education is 75.6 and for the 25th percentile of Catholic and 75th percentile of Education is 64.7. This indicates that lower education and stronger Catholic values are associated with increased fertility rates.

### Q1.6
An interpretation of the meaning of the model by writing a scientific abstract. (<150 words) (10pts)

  + BACKGROUND: Fertility rates across regions in Switzerland have historically been influenced by socio-cultural and economic factors. Prior studies suggest that religious affiliation and education may play significant roles in shaping demographic patterns.
  
  + OBJECTIVE: To model fertility levels in Swiss provinces using socioeconomic predictors and evaluate their relative influence.
  
  + METHODS: We analyzed the built-in swiss dataset, using fertility as the outcome variable. A multiple linear regression model was constructed with predictors including Agriculture, Education, Catholic affiliation, and Infant Mortality. Stepwise AIC was used for variable selection, and model diagnostics were performed to verify assumptions.
  
  + RESULTS: The final model revealed that higher Catholic affiliation and lower education levels were significantly associated with higher fertility rates. Model assumptions were reasonably satisfied. Predictions across realistic covariate values reflected these relationships
  
  + CONCLUSIONS: Fertility in Swiss regions is strongly associated with cultural and educational factors. These results underscore the demographic impact of religion and education in shaping fertility patterns. Further research is needed to explore the underlying mechanisms and implications for public policy.


## Q2.(70pts)  

The National Institute of Diabetes and Digestive and Kidney Diseases conducted a study on 768 adult female Pima Indians living near Phoenix. The purpose of the study was to investigate factors related to diabetes. The data may be found in the the dataset `pima`.

### Q2.1 

Create a factor version of the test results and use this to produce an interleaved histogram to show how the distribution of insulin differs between those testing positive and negative. Do you notice anything unbelievable about the plot? (5pts)
**Solution:**
```{r}
data(pima)
str(pima)
summary(pima)
# Convert `test` to a factor
pima <- pima %>%
  mutate(test_factor = factor(test, labels = c("Negative", "Positive")))

# Interleaved histogram
ggplot(pima, aes(x = insulin, fill = test_factor)) +
  geom_histogram(position = "identity", alpha = 0.5, bins = 30) +
  labs(
    title = "Distribution of Insulin by Diabetes Test Result",
    x = "Insulin",
    y = "Count",
    fill = "Test Result"
  ) +
  theme_minimal()
```
The histogram reveals that a large number of individuals, both diabetic and non-diabetic, have insulin values recorded as 0. This is highly unlikely in real biological measurements, indicating that these values likely represent missing data.

### Q2.2

Replace the zero values of `insulin` with the missing value code `NA`. Recreatethe interleaved histogram plot and comment on the distribution. (5pts)
**Solution:**
```{r}
# insulin = 0 as NA
pima <- pima %>%
  mutate(insulin_na = ifelse(insulin == 0, NA, insulin))

# Plot histogram using the new variable
ggplot(pima, aes(x = insulin_na, fill = test_factor)) +
  geom_histogram(position = "identity", alpha = 0.5, bins = 30) +
  labs(
    title = "Insulin Distribution (Excludes Zero Values)",
    x = "Insulin",
    y = "Count",
    fill = "Test Result"
  ) +
  theme_minimal()

```
The spike at zero has been eliminated, revealing that individuals who tested positive for diabetes tend to have higher and more variable insulin levels. Positive test group shows a broader and more right-skewed distribution. Negative group is tightly clustered at lower insulin values.


### Q2.3

Replace the incredible zeroes in other variables with the missing value code. Fit a model with the result of the diabetes test as the response and all the other variables as predictors. How many observations were used in the model fitting? Why is this less than the number of observations in the data frame. (10pts)
**Solution:**
```{r}
# New dataset with NA-coded vars
pima_clean <- pima %>%
  mutate(
    glucose = na_if(glucose, 0),
    diastolic = na_if(diastolic, 0),
    triceps = na_if(triceps, 0),
    insulin = na_if(insulin, 0),
    bmi = na_if(bmi, 0)
  )

# Fit logistic regression with all predictors
glm_full <- glm(test ~ pregnant + glucose + diastolic + triceps +
                  insulin + bmi + diabetes + age,
                data = pima_clean,
                family = binomial())

summary(glm_full)

# Number of observations used in the model fitting
nrow(glm_full$model) 
```
The number of observations used in the model is fewer than the original dataset because rows with any missing values in the predictors are automatically dropped (glm() uses na.omit() by default). Since multiple variables had zeroes coded as missing, this reduced the usable sample size.
The model identified glucose, BMI, and diabetes pedigree as significant predictors of diabetes test results. 


### Q2.4

Refit the model but now without the insulin and triceps predictors. How many observations were used in fitting this model? Devise a test to compare this model with that in the previous question. (10pts)
**Solution:**
```{r}
# Get the row indices used in glm_full
used_rows <- as.numeric(rownames(glm_full$model))

# Refit reduced model
glm_reduced <- glm(test ~ pregnant + glucose + diastolic + bmi + diabetes + age,
                   data = pima_clean[used_rows, ],
                   family = binomial())
summary(glm_reduced)

# Number of observations used
nrow(glm_reduced$model) 

# Compare full vs. reduced model using LRT
anova(glm_reduced, glm_full, test = "Chisq")
```
To compare the full and reduced models, I specified the same rows used in the full model when fitting the reduced one. This was necessary because the likelihood ratio test (anova()) requires both models to be fit on the exact same data.\
The likelihood ratio test gave a p-value of 0.6507, indicating that dropping insulin and triceps did not significantly worsen the model. Therefore, the reduced model is preferred for being simpler with no loss in predictive power.

### Q2.5

Use AIC to select a model. You will need to take account of the missing values. Which predictors are selected? How many cases are used in your selected model? (10pts)

### Q2.6

Create a variable that indicates whether the case contains a missing value. Use this variable as a predictor of the test result. Is missingness associated with the test result? Refit the selected model, but now using as much of the data as reasonable. Explain why it is appropriate to do this. (10pts)

```{r}
library(faraway)
library(tidyverse)

pima <- pima %>% 
  mutate(
    glucose2  = ifelse(glucose == 0, NA, glucose),
    diastolic2 = ifelse(diastolic == 0, NA, diastolic),
    triceps2 = ifelse(triceps == 0, NA, triceps),
    insulin2 = ifelse(insulin == 0, NA, insulin),
    bmi2 = ifelse(bmi == 0, NA, bmi), 
    diabetes2 = ifelse(diabetes == 0, NA, diabetes),
    age2 = ifelse(age == 0, NA, age))

pima$missingNA = ifelse(apply(is.na(dplyr::select(pima, contains("2"))), 1, sum) > 0, 1, 0)

missing.glm <- glm(test ~ missingNA, family = binomial(), data = pima)

library(gtsummary)
missing.glm %>%
  tbl_regression() %>%
  bold_labels() %>%
  bold_p(t = 0.05)
```
From above regression, we found missingness was not associate with outcome. This means that the distribution of outcome when removing data with missing is still a representative of the original distribution. This justifies the use of "complete case" analysis. 

### Q2.7 

Using the last fitted model of the previous question, compute the odds ratio of testing positive for diabetes for a woman with a BMI at the first quartile compared with a woman at the third quartile, assuming that all other factors are held constant? Give a confidence interval for this odds ratio.(10pts)

### Q2.8 

Do women who test positive have higher diastolic blood pressures? Is the diastolic blood pressure significant in the regression model? Explain the distinction between the two questions and discuss why the answers are only apparently contradictory. (10pts)